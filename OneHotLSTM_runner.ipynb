{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb83877dd70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import seaborn as sns\n",
    "from scipy.stats import binom_test\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.utils.data\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from OneHotLSTM import *\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./dat/network_input_uniq_allele_All/train_EL1.txt' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0c2d7cbab243>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Load data and embeddings - remember to set the paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdatDir_ligands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./dat/network_input_uniq_allele_All/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_ligs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatDir_ligands\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train_EL1.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Peptide'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Allele'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_ligs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatDir_ligands\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test_EL1.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Peptide'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Allele'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'./dat/network_input_uniq_allele_All/train_EL1.txt' does not exist"
     ]
    }
   ],
   "source": [
    "#Load data and embeddings - remember to set the paths\n",
    "datDir_ligands = './dat/network_input/'\n",
    "train_ligs = pd.read_csv(os.path.join(datDir_ligands,'train.txt'),sep='\\t',names=['Peptide','Allele'])\n",
    "test_ligs = pd.read_csv(os.path.join(datDir_ligands,'test.txt'),sep='\\t',names=['Peptide','Allele'])\n",
    "\n",
    "datDir_embedding = './dat/embedding/'\n",
    "df_embedding = pd.read_csv(os.path.join(datDir_embedding,'aa_embedding_window5_dim100.txt'),header=None,comment='#')\n",
    "embeddingTensor = torch.tensor(df_embedding.loc[:,1:].values)\n",
    "\n",
    "vocab_size=embeddingTensor.size()[0]\n",
    "EMBEDDING_DIM=embeddingTensor.size()[1]\n",
    "HIDDEN_DIM = 50\n",
    "BATCH_SIZE = 10\n",
    "trainDatNum = 1000\n",
    "testDatNum = 500\n",
    "\n",
    "datAlleles = [group[0] for group in train_ligs.groupby('Allele') if len(group[1])>300]#Include all alleles with more than 300 ligands associated\n",
    "\n",
    "#Upsample the predefined number of ligands from each allele to build balanced datasets\n",
    "train_ligs_select_sample = selectXnumOfClass(train_ligs,'Allele',datAlleles,trainDatNum)\n",
    "test_ligs_select_sample = selectXnumOfClass(test_ligs,'Allele',datAlleles,testDatNum)\n",
    "\n",
    "training_data,AA2IDX,MHC2IDX,IDX2MHC = initDataWrapper(train_ligs_select_sample,df_embedding,'Allele')\n",
    "test_data = DF2Dat(test_ligs_select_sample)\n",
    "\n",
    "#Create data iterators\n",
    "train_iter = torch.utils.data.DataLoader(training_data,batch_size=BATCH_SIZE)\n",
    "test_iter = torch.utils.data.DataLoader(test_data,batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize and train models\n",
    "\n",
    "model = lig2allele(EMBEDDING_DIM,HIDDEN_DIM,len(AA2IDX),len(MHC2IDX),BATCH_SIZE,bidirect=True)\n",
    "print({p[0]: p[1].requires_grad for p in model.named_parameters()})\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()),lr=0.01)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in train_iter:\n",
    "        #print(i[0])\n",
    "        inputs = list(map(lambda x:prepare_sequence(x,AA2IDX),i[0]))\n",
    "        inputs=torch.stack(inputs)\n",
    "        tag_scores = model(inputs)\n",
    "        #print(tag_scores)\n",
    "        break\n",
    "\n",
    "numEpochs=300\n",
    "epochCount = 5\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "testLoss = []\n",
    "\n",
    "for epoch in range(numEpochs):\n",
    "    if epoch%epochCount==0:\n",
    "        print('################Epoch%d/%d#################'%(epoch,numEpochs))\n",
    "    for sentence,alleles in train_iter:\n",
    "        #Step 1. Remember that Pytorch accumulates gradients\n",
    "        #We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "        #Also, we need to clear out the hidden state of the LSTM,\n",
    "        #detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "        #Step 2. Get our inputs ready for the networks, that is, turn them into,\n",
    "        #Tensors of word indices\n",
    "        sentence_in = list(map(lambda x:prepare_sequence(x,AA2IDX),sentence))\n",
    "        sentence_in=torch.stack(sentence_in)\n",
    "        #print(sentence_in.size())\n",
    "        if sentence_in.size()[1]!=BATCH_SIZE:\n",
    "            break\n",
    "        #sentence_in = prepare_sequence(sentence,AA2IDX)\n",
    "        targets = list(map(lambda x:prepare_sequence(x,MHC2IDX),alleles))[0]\n",
    "        #Step 3. Run our forwards pass.\n",
    "        tag_scores=model(sentence_in)\n",
    "        #Step 4. Compute the loss, gradients, and upadte the parameters\n",
    "        # by calling optimzer.step()\n",
    "        pred_IDX = torch.argmax(tag_scores,dim=1)\n",
    "        preds = list(map(lambda x:IDX2MHC[x.item()],list(pred_IDX)))\n",
    "        targPred = list(zip(alleles[0],preds))\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch%epochCount==0:\n",
    "        train_loss_val,train_targPred,train_accuracy = evaluateModel(train_iter)\n",
    "        test_loss_val,test_targPred,test_accuracy = evaluateModel(test_iter)\n",
    "        train_loss.append(train_loss_val)\n",
    "        test_loss.append(test_loss_val)\n",
    "        train_acc.append(train_accuracy)\n",
    "        test_acc.append(test_accuracy)\n",
    "        #testLoss.append(getLoss(model,test_iter))\n",
    "        print(\"Train Loss: %.3f, Test Loss: %.3f,Train Accuracy: %.3f, Test Accuracy: %.3f\"\n",
    "              %(train_loss_val,test_loss_val,train_accuracy,test_accuracy))\n",
    "        #print(\"Test Loss: %.3f\"%test_loss_val)\n",
    "        \n",
    "#testPred = evaluateModel(model,test_iter)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print training curve\n",
    "\n",
    "epochX = np.linspace(epochCount,numEpochs,(numEpochs/float(epochCount)))\n",
    "figDir_deepLearning = '/Users/birey/Dropbox/2018_TCell_PhD/Worklog/fig/dl/'\n",
    "\n",
    "plt.scatter(epochX,train_loss,c='b',label='Train')\n",
    "plt.scatter(epochX,test_loss,c='r',label='Test')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('NLLLoss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(epochX,train_acc,c='b',label='Train')\n",
    "plt.scatter(epochX,test_acc,c='xkcd:light orange',label='Test')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Confusion matrix at different levels of resolution for allele assignment\n",
    "\n",
    "train_targPred_superType = targPredCut(train_targPred,idx=-3)\n",
    "test_targPred_superType = targPredCut(test_targPred,idx=-3)\n",
    "\n",
    "train_targPred_locus = targPredCut(train_targPred,idx=-6)\n",
    "test_targPred_locus = targPredCut(test_targPred,idx=-6)\n",
    "\n",
    "figDir_deepLearning = './'\n",
    "conMat_train_full = confusionMatrix(train_targPred)\n",
    "conMat_test_full = confusionMatrix(test_targPred,labels=True,ticker=5,titleFont=40,saveFig=os.path.join(figDir_deepLearning,'confMat_fullRes.eps'))\n",
    "\n",
    "conMat_train_super = confusionMatrix(train_targPred_superType)\n",
    "conMat_test_super = confusionMatrix(test_targPred_superType,labels=True,titleFont=40,ticker=5,saveFig=os.path.join(figDir_deepLearning,'confMat_superType.eps'))\n",
    "\n",
    "conMat_train_locus = confusionMatrix(train_targPred_locus)\n",
    "conMat_test_locus = confusionMatrix(test_targPred_locus,labels=True,titleFont=30,saveFig=os.path.join(figDir_deepLearning,'confMat_locus.eps'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
